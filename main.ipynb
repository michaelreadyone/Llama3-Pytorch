{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:11:22.605420Z",
     "iopub.status.busy": "2024-09-14T11:11:22.604988Z",
     "iopub.status.idle": "2024-09-14T11:11:25.648079Z",
     "shell.execute_reply": "2024-09-14T11:11:25.646951Z",
     "shell.execute_reply.started": "2024-09-14T11:11:22.605354Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\miniconda3\\envs\\H2O\\lib\\site-packages\\torch\\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_tensor_type(torch.BFloat16Tensor) # Otherwise it may not fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:11:25.650358Z",
     "iopub.status.busy": "2024-09-14T11:11:25.649831Z",
     "iopub.status.idle": "2024-09-14T11:11:25.656441Z",
     "shell.execute_reply": "2024-09-14T11:11:25.655378Z",
     "shell.execute_reply.started": "2024-09-14T11:11:25.650298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DIM = 4096 # Llama3 Table 3.\n",
    "FFN_DIM = 14336 # For POSITION-WISE FEED-FORWARD NETWORK (FFN) Llama Table 3\n",
    "N_LAYERS = 32 # Llama3 Table 3.\n",
    "N_HEADS = 32 # Llama3 Table 3.\n",
    "N_KV_HEADS = 8 # With 8 key-value heads to improve inference speed and to reduce the size (llama3)\n",
    "VOCAB_SIZE = 128256 # Llama3 vocab size. Llama3 paper says 128K. This is the exact number taken from tokenizer.\n",
    "NORM_EPS = 1e-5 # Took from Llama3 code ModelArgs.\n",
    "ROPE_THETA = 500000 # We increase the RoPE base frequency hyperparameter to 500,000 (llama3)\n",
    "MAX_BATCH_SIZE = 4 # Just optional depending on your specs. If number of examples you provide is smaller, it takes it as batch size.\n",
    "MAX_SEQ_LEN = 128 # Just optional depending on your specs.\n",
    "N_KV_HEAD_REP = N_HEADS // N_KV_HEADS # How many times you repeat KV to match your queries(N_HEADS).\n",
    "HEAD_DIM = DIM // N_HEADS # Divide dimension by number of heads to get dimension per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:11:25.659602Z",
     "iopub.status.busy": "2024-09-14T11:11:25.659211Z",
     "iopub.status.idle": "2024-09-14T11:11:25.794218Z",
     "shell.execute_reply": "2024-09-14T11:11:25.792424Z",
     "shell.execute_reply.started": "2024-09-14T11:11:25.659558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4096])\n",
      "------------------------------\n",
      "torch.Size([2, 8, 32, 128])\n",
      "torch.Size([2, 8, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "# Apply pre-normalization using RMSNorm (llama2)\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, norm_eps):\n",
    "        super().__init__()\n",
    "        self.norm_eps = norm_eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self._norm(x.float()).type_as(x)\n",
    "        return out * self.weight # (2, 8, DIM) Values stays the same. We make the tensor grad_fn.\n",
    "    \n",
    "dummy_inp = torch.randn(2, 8, DIM)\n",
    "norm = RMSNorm(DIM, NORM_EPS)\n",
    "output = norm(dummy_inp)\n",
    "print(output.shape)\n",
    "\n",
    "def precompute_freqs_cis(dim, end, theta = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis, x):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(xq, xk, freqs_cis):\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "dummy_inp1 = torch.randn(2, 8, N_HEADS, HEAD_DIM)\n",
    "dummy_inp2 = torch.randn(2, 8, N_KV_HEADS, HEAD_DIM)\n",
    "\n",
    "dummy_freqs_cis = precompute_freqs_cis(HEAD_DIM, MAX_SEQ_LEN*2, ROPE_THETA)\n",
    "dummy_freqs_cis = dummy_freqs_cis[0 : 0 + 8]\n",
    "\n",
    "out1, out2 = apply_rotary_emb(dummy_inp1, dummy_inp2, dummy_freqs_cis)\n",
    "print(\"-\"*30)\n",
    "print(out1.shape)\n",
    "print(out2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:11:25.795979Z",
     "iopub.status.busy": "2024-09-14T11:11:25.795604Z",
     "iopub.status.idle": "2024-09-14T11:11:27.812286Z",
     "shell.execute_reply": "2024-09-14T11:11:27.811427Z",
     "shell.execute_reply.started": "2024-09-14T11:11:25.795938Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4096])\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Bias is false. It usually adds overhead to the transformer models.\n",
    "        self.w1 = nn.Linear(DIM, FFN_DIM, bias=False)\n",
    "        self.w3 = nn.Linear(DIM, FFN_DIM, bias=False)\n",
    "        self.w2 = nn.Linear(FFN_DIM, DIM, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x)) # (2, 8, DIM) = (bsz, seqlen, DIM) - use the SwiGLU activation function (llama3) Table 3.\n",
    "    \n",
    "dummy_inp = torch.randn(2, 8, DIM)\n",
    "\n",
    "feed_forward = FeedForward()\n",
    "\n",
    "output = feed_forward(dummy_inp)\n",
    "del feed_forward\n",
    "print(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:11:27.814723Z",
     "iopub.status.busy": "2024-09-14T11:11:27.813978Z",
     "iopub.status.idle": "2024-09-14T11:11:28.727891Z",
     "shell.execute_reply": "2024-09-14T11:11:28.727010Z",
     "shell.execute_reply.started": "2024-09-14T11:11:27.814678Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4096])\n"
     ]
    }
   ],
   "source": [
    "# GQA With Cache\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(DIM, N_HEADS * HEAD_DIM, bias=False)\n",
    "        self.wk = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False)\n",
    "        self.wv = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False)\n",
    "        self.wo = nn.Linear(N_HEADS * HEAD_DIM, DIM, bias=False) # Weight matrix defined in the MultiheadAttention formula.\n",
    "\n",
    "        # Create empty caches for keys and values.\n",
    "        self.cache_k = torch.zeros(\n",
    "            (\n",
    "                MAX_BATCH_SIZE,\n",
    "                MAX_SEQ_LEN,\n",
    "                N_KV_HEADS,\n",
    "                HEAD_DIM,\n",
    "            )\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (\n",
    "                MAX_BATCH_SIZE,\n",
    "                MAX_SEQ_LEN,\n",
    "                N_KV_HEADS,\n",
    "                HEAD_DIM,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x, start_pos, freqs_cis, mask):\n",
    "        bsz, seqlen, _ = x.shape # Get batch size and sequence length. (bsz, seqlen, DIM)\n",
    "        queries, keys, values = self.wq(x), self.wk(x), self.wv(x) # q -> (bsz, seqlen, N_HEADS*HEAD_DIM) | k,v -> (bsz, seqlen, N_KV_HEADS*HEAD_DIM)\n",
    "\n",
    "        queries = queries.view(bsz, seqlen, N_HEADS, HEAD_DIM)\n",
    "        keys = keys.view(bsz, seqlen, N_KV_HEADS, HEAD_DIM)\n",
    "        values = values.view(bsz, seqlen, N_KV_HEADS, HEAD_DIM)\n",
    "\n",
    "        queries, keys = apply_rotary_emb(queries, keys, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(queries.device)\n",
    "        self.cache_v = self.cache_v.to(queries.device)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = keys\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = values\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        # In these runs we simply duplicated the KV heads for MQA in all GPUs. (llama2)\n",
    "        keys = torch.repeat_interleave(\n",
    "            keys, dim=2, repeats=N_KV_HEAD_REP\n",
    "        ) # (bsz, seqlen, N_KV_HEADS, HEAD_DIM) -> (bsz, seqlen, N_HEADS, HEAD_DIM)\n",
    "        values = torch.repeat_interleave(\n",
    "            values, dim=2, repeats=N_KV_HEAD_REP\n",
    "        )  # (bsz, seqlen, N_KV_HEADS, HEAD_DIM) -> (bsz, seqlen, N_HEADS, HEAD_DIM)\n",
    "\n",
    "        # Reshaping for scaled_dot_product_attention. (bsz, ..., seqlen, HEAD_DIM) expected.\n",
    "        queries = queries.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n",
    "        keys = keys.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n",
    "        values = values.transpose(1, 2) # (bsz, seqlen, N_HEADS, HEAD_DIM) -> (bsz, N_HEADS, seqlen, HEAD_DIM)\n",
    "\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask=mask,\n",
    "        ) # (bsz, N_HEADS, seqlen, HEAD_DIM)\n",
    "        \n",
    "        \n",
    "        # If we don't use `contiguous` torch may complain.\n",
    "        out = out.transpose(1, 2).contiguous().view(bsz, seqlen, -1) # transpose, (bsz, seqlen, N_HEADS, HEAD_DIM) - (bsz, seqlen, DIM) - -1 does N_HEAD * HEAD_DIM = DIM\n",
    "        return self.wo(out) # (bsz, seqlen, DIM)\n",
    "    \n",
    "dummy_inp = torch.randn(2, 8, DIM) # 8 is sequence length. Depends on your input. I put 8.\n",
    "dummy_start_pos = 0\n",
    "dummy_freqs_cis = torch.randn(8, 64)\n",
    "dummy_mask = torch.randn(8, 8) # Mask is size (seqlen, seqlen)\n",
    "\n",
    "attention = Attention()\n",
    "\n",
    "output = attention(dummy_inp, dummy_start_pos, dummy_freqs_cis, dummy_mask)\n",
    "print(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)\n",
    "del attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:11:28.730185Z",
     "iopub.status.busy": "2024-09-14T11:11:28.729478Z",
     "iopub.status.idle": "2024-09-14T11:11:31.398547Z",
     "shell.execute_reply": "2024-09-14T11:11:31.397556Z",
     "shell.execute_reply.started": "2024-09-14T11:11:28.730142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4096])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention = Attention()\n",
    "        self.feed_forward = FeedForward()\n",
    "        self.attention_norm = RMSNorm(DIM, NORM_EPS)\n",
    "        self.ffn_norm = RMSNorm(DIM, NORM_EPS)\n",
    "\n",
    "    def forward(self, x, start_pos, freqs_cis, mask):\n",
    "        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask) # (2, 8, 4096) = (bsz, seqlen, DIM)\n",
    "        out = h + self.feed_forward(self.ffn_norm(h)) # (2, 8, DIM) = (bsz, seqlen, DIM)\n",
    "        return out # (2, 8, DIM) = (bsz, seqlen, DIM)\n",
    "    \n",
    "dummy_inp = torch.randn(2, 8, DIM)\n",
    "dummy_start_pos = 0\n",
    "dummy_freqs_cis = torch.randn(8, 64)\n",
    "dummy_mask = torch.randn(8, 8)\n",
    "\n",
    "transformer_block = TransformerBlock()\n",
    "\n",
    "output = transformer_block(dummy_inp, dummy_start_pos, dummy_freqs_cis, dummy_mask)\n",
    "print(output.shape) # (2, 8, DIM) = (bsz, seqlen, DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:11:31.400608Z",
     "iopub.status.busy": "2024-09-14T11:11:31.399968Z",
     "iopub.status.idle": "2024-09-14T11:13:05.999896Z",
     "shell.execute_reply": "2024-09-14T11:13:05.998799Z",
     "shell.execute_reply.started": "2024-09-14T11:11:31.400553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 128256])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tok_embeddings = nn.Embedding(\n",
    "            VOCAB_SIZE, DIM\n",
    "        )\n",
    "        \n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for _ in range(N_LAYERS):\n",
    "            self.layers.append(TransformerBlock())\n",
    "\n",
    "        self.norm = RMSNorm(DIM, NORM_EPS)\n",
    "        self.output = nn.Linear(DIM, VOCAB_SIZE, bias=False,)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            HEAD_DIM,\n",
    "            MAX_SEQ_LEN * 2,\n",
    "            ROPE_THETA,\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens, start_pos):       \n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens) # (bsz, seqlen, DIM)\n",
    "        self.freqs_cis = self.freqs_cis.to(tokens.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None # When we take the tokens from the cached values (seqlen=1) we don't need any aditional mask.\n",
    "        if seqlen > 1: # Because of KV Cache, we process only 1 token. However, the first run doesn't have any cache. So it has a seqlen > 1.\n",
    "            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device) # Since this is the first pass, we don't have any KV Cache. So we need a mask. Create (seqlen, seqlen) matrix with float(\"-inf\") values.\n",
    "\n",
    "            mask = torch.triu(mask, diagonal=1).to(tokens.device) # Take the upper triangle excluding diagonal since it's casual LM.\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask) # (2, 8, 4096) = (bsz, seqlen, DIM)\n",
    "        h = self.norm(h) # (2, 8, 4096) = (bsz, seqlen, DIM)\n",
    "        out = self.output(h).float() # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n",
    "        return out # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n",
    "    \n",
    "dummy_tokens = torch.rand(2, 8).long() # Use `rand` instead of `randn`. `randn` generates negative number which is invalid for `nn.Embedding`\n",
    "dummy_start_pos = 0\n",
    "\n",
    "transformer = Transformer()\n",
    "\n",
    "output = transformer(dummy_tokens, dummy_start_pos)\n",
    "print(output.shape) # (2, 8, 128256) = (bsz, seqlen, VOCAB_SIZE)\n",
    "del transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:13:06.002979Z",
     "iopub.status.busy": "2024-09-14T11:13:06.002641Z",
     "iopub.status.idle": "2024-09-14T11:13:06.169956Z",
     "shell.execute_reply": "2024-09-14T11:13:06.168958Z",
     "shell.execute_reply.started": "2024-09-14T11:13:06.002944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
    "\n",
    "import os\n",
    "from logging import getLogger\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "\n",
    "class Message(TypedDict):\n",
    "    role: Role\n",
    "    content: str\n",
    "\n",
    "\n",
    "Dialog = Sequence[Message]\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    special_tokens: Dict[str, int]\n",
    "\n",
    "    num_reserved_special_tokens = 256\n",
    "\n",
    "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the Tokenizer with a Tiktoken model.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The path to the Tiktoken model file.\n",
    "        \"\"\"\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "        num_base_tokens = len(mergeable_ranks)\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [\n",
    "            f\"<|reserved_special_token_{i}|>\"\n",
    "            for i in range(5, self.num_reserved_special_tokens - 5)\n",
    "        ]\n",
    "        self.special_tokens = {\n",
    "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens,\n",
    "        )\n",
    "        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n",
    "\n",
    "        self.n_words: int = self.model.n_vocab\n",
    "        # BOS / EOS token IDs\n",
    "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
    "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
    "        self.pad_id: int = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens[\"<|end_of_text|>\"],\n",
    "            self.special_tokens[\"<|eot_id|>\"],\n",
    "        }\n",
    "        logger.info(\n",
    "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
    "        )\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        *,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encodes a string into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            s (str): The input string to be encoded.\n",
    "            bos (bool): Whether to prepend the beginning-of-sequence token.\n",
    "            eos (bool): Whether to append the end-of-sequence token.\n",
    "            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n",
    "            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n",
    "\n",
    "        Returns:\n",
    "            list[int]: A list of token IDs.\n",
    "\n",
    "        By default, setting disallowed_special=() encodes a string by ignoring\n",
    "        special tokens. Specifically:\n",
    "        - Setting `disallowed_special` to () will cause all text corresponding\n",
    "          to special tokens to be encoded as natural text (insteading of raising\n",
    "          an error).\n",
    "        - Setting `allowed_special` to \"all\" will treat all text corresponding\n",
    "          to special tokens to be encoded as special tokens.\n",
    "        \"\"\"\n",
    "        assert type(s) is str\n",
    "\n",
    "        # The tiktoken tokenizer can handle <=400k chars without\n",
    "        # pyo3_runtime.PanicException.\n",
    "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
    "\n",
    "        # https://github.com/openai/tiktoken/issues/195\n",
    "        # Here we iterate over subsequences and split if we exceed the limit\n",
    "        # of max consecutive non-whitespace or whitespace characters.\n",
    "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
    "\n",
    "        substrs = (\n",
    "            substr\n",
    "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
    "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
    "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
    "            )\n",
    "        )\n",
    "        t: List[int] = []\n",
    "        for substr in substrs:\n",
    "            t.extend(\n",
    "                self.model.encode(\n",
    "                    substr,\n",
    "                    allowed_special=allowed_special,\n",
    "                    disallowed_special=disallowed_special,\n",
    "                )\n",
    "            )\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: Sequence[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs into a string.\n",
    "\n",
    "        Args:\n",
    "            t (List[int]): The list of token IDs to be decoded.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n",
    "        return self.model.decode(cast(List[int], t))\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_whitespaces_or_nonwhitespaces(\n",
    "        s: str, max_consecutive_slice_len: int\n",
    "    ) -> Iterator[str]:\n",
    "        \"\"\"\n",
    "        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n",
    "        consecutive whitespaces or consecutive non-whitespaces.\n",
    "        \"\"\"\n",
    "        current_slice_len = 0\n",
    "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
    "        slice_start = 0\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            is_now_space = s[i].isspace()\n",
    "\n",
    "            if current_slice_is_space ^ is_now_space:\n",
    "                current_slice_len = 1\n",
    "                current_slice_is_space = is_now_space\n",
    "            else:\n",
    "                current_slice_len += 1\n",
    "                if current_slice_len > max_consecutive_slice_len:\n",
    "                    yield s[slice_start:i]\n",
    "                    slice_start = i\n",
    "                    current_slice_len = 1\n",
    "        yield s[slice_start:]\n",
    "\n",
    "\n",
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer: Tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message: Message) -> List[int]:\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode_message(self, message: Message) -> List[int]:\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def encode_dialog_prompt(self, dialog: Dialog) -> List[int]:\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n",
    "        for message in dialog:\n",
    "            tokens.extend(self.encode_message(message))\n",
    "        # Add the start of an assistant message for the model to complete.\n",
    "        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n",
    "        return tokens\n",
    "\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed in accordance with the terms of the Llama 3 Community License Agreement.\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, TypedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CompletionPrediction(TypedDict, total=False):\n",
    "    generation: str\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "class ChatPrediction(TypedDict, total=False):\n",
    "    generation: Message\n",
    "    tokens: List[str]  # not required\n",
    "    logprobs: List[float]  # not required\n",
    "\n",
    "\n",
    "class Llama:\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        ckpt_dir: str,\n",
    "        tokenizer_path: str,\n",
    "        max_seq_len: int,\n",
    "        max_batch_size: int,\n",
    "        model_parallel_size: Optional[int] = None,\n",
    "        seed: int = 1,\n",
    "    ) -> \"Llama\":\n",
    "        \"\"\"\n",
    "        Build a Llama instance by initializing and loading a model checkpoint.\n",
    "\n",
    "        Args:\n",
    "            ckpt_dir (str): Path to the directory containing checkpoint files.\n",
    "            tokenizer_path (str): Path to the tokenizer file.\n",
    "            max_seq_len (int): Maximum sequence length for input text.\n",
    "            max_batch_size (int): Maximum batch size for inference.\n",
    "            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n",
    "                If not provided, it's determined from the environment. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Llama: An instance of the Llama class with the loaded model and tokenizer.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If there are no checkpoint files in the specified directory,\n",
    "                or if the model parallel size does not match the number of checkpoint files.\n",
    "\n",
    "        Note:\n",
    "            This method initializes the distributed process group, sets the device to CUDA,\n",
    "            and loads the pre-trained model and tokenizer.\n",
    "        \"\"\"\n",
    "        assert 1 <= max_seq_len <= 8192, f\"max_seq_len must be between 1 and 8192, got {max_seq_len}.\"\n",
    "        assert os.path.isdir(ckpt_dir), f\"Checkpoint directory '{ckpt_dir}' does not exist.\"\n",
    "        assert os.path.isfile(tokenizer_path), f\"Tokenizer file '{tokenizer_path}' does not exist.\"\n",
    "\n",
    "        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "        torch.cuda.set_device(local_rank)\n",
    "\n",
    "        # seed must be the same in all processes\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        if local_rank > 0:\n",
    "            sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n",
    "        checkpoint = torch.load(ckpt_dir+\"consolidated.00.pth\", map_location=\"cpu\")\n",
    "        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "            params = json.loads(f.read())\n",
    "            \n",
    "        tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "        assert VOCAB_SIZE == tokenizer.n_words\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n",
    "        else:\n",
    "            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "        model = Transformer()\n",
    "        print(f\"PARAMETERS: {sum(p.numel() for p in model.parameters())}\")\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "        return Llama(model, tokenizer)\n",
    "\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.formatter = ChatFormat(tokenizer)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt_tokens: List[List[int]],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n",
    "        \"\"\"\n",
    "        Generate text sequences based on provided prompts using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n",
    "            max_gen_len (int): Maximum length of the generated text sequence.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n",
    "\n",
    "        Note:\n",
    "            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "\n",
    "        \"\"\"\n",
    "        bsz = len(prompt_tokens)\n",
    "        assert bsz <= MAX_BATCH_SIZE, (bsz, MAX_BATCH_SIZE)\n",
    "\n",
    "        min_prompt_len = min(len(t) for t in prompt_tokens)\n",
    "        max_prompt_len = max(len(t) for t in prompt_tokens)\n",
    "        assert max_prompt_len <= MAX_SEQ_LEN\n",
    "        total_len = min(MAX_SEQ_LEN, max_gen_len + max_prompt_len)\n",
    "\n",
    "        pad_id = self.tokenizer.pad_id\n",
    "        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n",
    "        if logprobs:\n",
    "            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "        prev_pos = 0\n",
    "        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n",
    "        input_text_mask = tokens != pad_id\n",
    "        if min_prompt_len == total_len:\n",
    "            logits = self.model.forward(tokens, prev_pos)\n",
    "            token_logprobs = -F.cross_entropy(\n",
    "                input=logits.transpose(1, 2),\n",
    "                target=tokens,\n",
    "                reduction=\"none\",\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "\n",
    "        stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n",
    "\n",
    "        for cur_pos in range(min_prompt_len, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
    "\n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            if logprobs:\n",
    "                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n",
    "                    input=logits.transpose(1, 2),\n",
    "                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n",
    "                    reduction=\"none\",\n",
    "                    ignore_index=pad_id,\n",
    "                )\n",
    "            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "                torch.isin(next_token, stop_tokens)\n",
    "            )\n",
    "            prev_pos = cur_pos\n",
    "            if all(eos_reached):\n",
    "                break\n",
    "\n",
    "        if logprobs:\n",
    "            token_logprobs = token_logprobs.tolist()\n",
    "        out_tokens, out_logprobs = [], []\n",
    "        for i, toks in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            start = 0 if echo else len(prompt_tokens[i])\n",
    "            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            probs = None\n",
    "            if logprobs:\n",
    "                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to after eos tok if any\n",
    "            for stop_token in self.tokenizer.stop_tokens:\n",
    "                try:\n",
    "                    eos_idx = toks.index(stop_token)\n",
    "                    toks = toks[:eos_idx]\n",
    "                    probs = probs[:eos_idx] if logprobs else None\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            out_tokens.append(toks)\n",
    "            out_logprobs.append(probs)\n",
    "        return (out_tokens, out_logprobs if logprobs else None)\n",
    "\n",
    "    def text_completion(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "        echo: bool = False,\n",
    "    ) -> List[CompletionPrediction]:\n",
    "        \"\"\"\n",
    "        Perform text completion for a list of prompts using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            prompts (List[str]): List of text prompts for completion.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n",
    "                If not provided, it's set to the model's maximum sequence length minus 1.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n",
    "\n",
    "        Note:\n",
    "            This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "\n",
    "        \"\"\"\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = MAX_SEQ_LEN - 1\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "            echo=echo,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": self.tokenizer.decode(t),\n",
    "                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]\n",
    "\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        dialogs: List[Dialog],\n",
    "        temperature: float = 0.6,\n",
    "        top_p: float = 0.9,\n",
    "        max_gen_len: Optional[int] = None,\n",
    "        logprobs: bool = False,\n",
    "    ) -> List[ChatPrediction]:\n",
    "        \"\"\"\n",
    "        Generate assistant responses for a list of conversational dialogs using the language generation model.\n",
    "\n",
    "        Args:\n",
    "            dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n",
    "            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n",
    "            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n",
    "            max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n",
    "                If not provided, it's set to the model's maximum sequence length minus 1.\n",
    "            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            List[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n",
    "\n",
    "        Note:\n",
    "            This method generates assistant responses for the provided conversational dialogs.\n",
    "            It employs nucleus sampling to introduce controlled randomness in text generation.\n",
    "            If logprobs is True, token log probabilities are computed for each generated token.\n",
    "        \"\"\"\n",
    "        if max_gen_len is None:\n",
    "            max_gen_len = MAX_SEQ_LEN - 1\n",
    "\n",
    "        prompt_tokens = [\n",
    "            self.formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n",
    "        ]\n",
    "        generation_tokens, generation_logprobs = self.generate(\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            max_gen_len=max_gen_len,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            logprobs=logprobs,\n",
    "        )\n",
    "        if logprobs:\n",
    "            return [\n",
    "                {\n",
    "                    \"generation\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": self.tokenizer.decode(t),\n",
    "                    },\n",
    "                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n",
    "                    \"logprobs\": logprobs_i,\n",
    "                }\n",
    "                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n",
    "            ]\n",
    "        return [\n",
    "            {\n",
    "                \"generation\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": self.tokenizer.decode(t),\n",
    "                },\n",
    "            }\n",
    "            for t in generation_tokens\n",
    "        ]\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    \"\"\"\n",
    "    Perform top-p (nucleus) sampling on a probability distribution.\n",
    "\n",
    "    Args:\n",
    "        probs (torch.Tensor): Probability distribution tensor.\n",
    "        p (float): Probability threshold for top-p sampling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Sampled token indices.\n",
    "\n",
    "    Note:\n",
    "        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n",
    "        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n",
    "    \"\"\"\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-14T11:15:54.329480Z",
     "iopub.status.busy": "2024-09-14T11:15:54.328545Z",
     "iopub.status.idle": "2024-09-14T11:18:45.597323Z",
     "shell.execute_reply": "2024-09-14T11:18:45.596318Z",
     "shell.execute_reply.started": "2024-09-14T11:15:54.329441Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS: 8030261248\n",
      "Loaded in 10.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_33692\\1906971230.py:60: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  out = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      ">  to be happy. I believe that it is the duty of every human being to be happy. I believe that the meaning of life is to be happy, and that happiness is the only thing that matters. I believe that happiness is the most important thing in the world, and that it is the only thing that truly matters\n",
      "\n",
      "==================================\n",
      "\n",
      "Simply put, the theory of relativity states that \n",
      "> 2 objects in motion with respect to each other will age at different rates. In Einstein’s theory, the faster an object is moving, the slower time will pass for that object.\n",
      "So, if you could travel at the speed of light, time would stand still for you. The faster you move, the slower time will\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"llama3\"\n",
    "tokenizer_path = \"llama3/tokenizer.model\"\n",
    "        \n",
    "temperature = 0.6\n",
    "top_p = 0.9\n",
    "max_seq_len = 128\n",
    "max_gen_len = 64\n",
    "max_batch_size = 4\n",
    "\n",
    "generator = Llama.build(\n",
    "    ckpt_dir=ckpt_dir,\n",
    "    tokenizer_path=tokenizer_path,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_batch_size=max_batch_size,\n",
    "    model_parallel_size=1\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "    # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "    \"I believe the meaning of life is\",\n",
    "    \"Simply put, the theory of relativity states that \",\n",
    "]\n",
    "results = generator.text_completion(\n",
    "    prompts,\n",
    "    max_gen_len=max_gen_len,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    ")\n",
    "for prompt, result in zip(prompts, results):\n",
    "    print(prompt)\n",
    "    print(f\"> {result['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5417429,
     "sourceId": 8994037,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "H2O",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
